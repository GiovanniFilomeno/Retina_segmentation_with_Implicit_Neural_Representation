{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 10.431130409240723\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 15.18659782409668\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 13.843182563781738\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 10.250557899475098\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 9.32960319519043\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.972789764404297\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.630720138549805\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.612722396850586\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.787675857543945\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.331501007080078\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.492801666259766\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.275425910949707\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.446663856506348\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.226096153259277\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.39275074005127\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.24657917022705\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.137227058410645\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.361371994018555\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 8.049650192260742\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 7.997747421264648\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 7.829991817474365\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 7.895421504974365\n",
      "Current learning rate: 0.001\n",
      "Epoch 0, Loss: 7.668073654174805\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 7.572843551635742\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 7.694291114807129\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 7.840767860412598\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 7.425364971160889\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 7.636386394500732\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 7.283334732055664\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 7.31157112121582\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 7.146849632263184\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 6.8950042724609375\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 7.37386417388916\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 7.372579574584961\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 6.928308486938477\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 6.832010269165039\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 6.597858905792236\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 6.24395227432251\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 6.114980220794678\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 6.638601303100586\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 5.930644512176514\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 5.885344505310059\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 5.982507705688477\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 5.824521064758301\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 5.93707275390625\n",
      "Current learning rate: 0.001\n",
      "Epoch 1, Loss: 5.3551836013793945\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 5.384275436401367\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 5.327181339263916\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 5.356851100921631\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 5.587424278259277\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 5.90847110748291\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 5.268312454223633\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 5.27180814743042\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 4.579559326171875\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 4.574891567230225\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 5.432997226715088\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 4.728611469268799\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 4.782133102416992\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 4.784393787384033\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 4.414680004119873\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 5.47597599029541\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 4.009949684143066\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 4.037700653076172\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 4.451592922210693\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 3.6592395305633545\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 4.800938606262207\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 3.9004201889038086\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 4.805761337280273\n",
      "Current learning rate: 0.001\n",
      "Epoch 2, Loss: 4.490629196166992\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 4.175442218780518\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 4.132776260375977\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 4.954951286315918\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 4.263676166534424\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 3.7945895195007324\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 3.5760536193847656\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 3.3729851245880127\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 3.6058359146118164\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 3.0327706336975098\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 3.749497413635254\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 4.021417617797852\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 4.237542629241943\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 3.2283239364624023\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 4.391267776489258\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 3.020207405090332\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 3.2517786026000977\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 3.960516929626465\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 4.121759414672852\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 4.966189861297607\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 4.481022357940674\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 3.3535988330841064\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 2.9199538230895996\n",
      "Current learning rate: 0.001\n",
      "Epoch 3, Loss: 4.553134918212891\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 3.0667052268981934\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 3.421302318572998\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 3.9830775260925293\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 2.9754438400268555\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 3.3367838859558105\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 3.9256396293640137\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 2.664572238922119\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 3.1074278354644775\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 4.153810024261475\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 4.5597944259643555\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 2.8832266330718994\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 4.483586311340332\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 3.212113857269287\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 3.549184560775757\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 4.093092918395996\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 4.788429260253906\n",
      "Current learning rate: 0.001\n",
      "Epoch 4, Loss: 3.9121382236480713\n",
      "Current learning rate: 0.0008\n",
      "Epoch 4, Loss: 3.3174591064453125\n",
      "Current learning rate: 0.0008\n",
      "Epoch 4, Loss: 3.735836982727051\n",
      "Current learning rate: 0.0008\n",
      "Epoch 4, Loss: 4.986887454986572\n",
      "Current learning rate: 0.0008\n",
      "Epoch 4, Loss: 4.336933612823486\n",
      "Current learning rate: 0.0008\n",
      "Epoch 4, Loss: 2.886514663696289\n",
      "Current learning rate: 0.0008\n",
      "Epoch 4, Loss: 3.697545289993286\n",
      "Current learning rate: 0.0008\n",
      "Epoch 5, Loss: 4.481086730957031\n",
      "Current learning rate: 0.0008\n",
      "Epoch 5, Loss: 3.2201857566833496\n",
      "Current learning rate: 0.0008\n",
      "Epoch 5, Loss: 2.8423287868499756\n",
      "Current learning rate: 0.0008\n",
      "Epoch 5, Loss: 3.108045816421509\n",
      "Current learning rate: 0.0008\n",
      "Epoch 5, Loss: 3.9208624362945557\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 4.08891487121582\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 3.7302513122558594\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 4.985321521759033\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 3.0498080253601074\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 3.9088032245635986\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 2.642134189605713\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 4.152414321899414\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 3.384495735168457\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 3.978809356689453\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 4.781407356262207\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 3.34020733833313\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 2.957371473312378\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 4.559725761413574\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 3.536585807800293\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 4.335689067840576\n",
      "Current learning rate: 0.00064\n",
      "Epoch 5, Loss: 3.2971651554107666\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 5, Loss: 2.852292060852051\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 5, Loss: 3.678539752960205\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 6, Loss: 2.7923898696899414\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 6, Loss: 3.365596294403076\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 6, Loss: 2.6096739768981934\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 6, Loss: 5.021381378173828\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 6, Loss: 2.9982268810272217\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 6, Loss: 2.9176738262176514\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 6, Loss: 4.501081466674805\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 6, Loss: 3.5343053340911865\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 6, Loss: 3.976668357849121\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 6, Loss: 4.802331447601318\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 6, Loss: 3.914689064025879\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 6, Loss: 3.113560676574707\n",
      "Current learning rate: 0.0005120000000000001\n",
      "Epoch 6, Loss: 3.8978281021118164\n",
      "Current learning rate: 0.0004096000000000001\n",
      "Epoch 6, Loss: 3.7225146293640137\n",
      "Current learning rate: 0.0004096000000000001\n",
      "Epoch 6, Loss: 4.143908500671387\n",
      "Current learning rate: 0.0004096000000000001\n",
      "Epoch 6, Loss: 3.6743600368499756\n",
      "Current learning rate: 0.0004096000000000001\n",
      "Epoch 6, Loss: 3.3069424629211426\n",
      "Current learning rate: 0.0004096000000000001\n",
      "Epoch 6, Loss: 2.8136098384857178\n",
      "Current learning rate: 0.0004096000000000001\n",
      "Epoch 6, Loss: 4.3383893966674805\n",
      "Current learning rate: 0.0004096000000000001\n",
      "Epoch 6, Loss: 4.085351943969727\n",
      "Current learning rate: 0.0004096000000000001\n",
      "Epoch 6, Loss: 3.2610275745391846\n",
      "Current learning rate: 0.0004096000000000001\n",
      "Epoch 6, Loss: 4.558671474456787\n",
      "Current learning rate: 0.0004096000000000001\n",
      "Epoch 6, Loss: 3.194469451904297\n",
      "Current learning rate: 0.0004096000000000001\n",
      "Epoch 7, Loss: 4.31725549697876\n",
      "Current learning rate: 0.0003276800000000001\n",
      "Epoch 7, Loss: 3.306553363800049\n",
      "Current learning rate: 0.0003276800000000001\n",
      "Epoch 7, Loss: 2.824234962463379\n",
      "Current learning rate: 0.0003276800000000001\n",
      "Epoch 7, Loss: 4.539321422576904\n",
      "Current learning rate: 0.0003276800000000001\n",
      "Epoch 7, Loss: 3.9022417068481445\n",
      "Current learning rate: 0.0003276800000000001\n",
      "Epoch 7, Loss: 3.2567880153656006\n",
      "Current learning rate: 0.0003276800000000001\n",
      "Epoch 7, Loss: 3.9125638008117676\n",
      "Current learning rate: 0.0003276800000000001\n",
      "Epoch 7, Loss: 2.6156373023986816\n",
      "Current learning rate: 0.0003276800000000001\n",
      "Epoch 7, Loss: 4.141855716705322\n",
      "Current learning rate: 0.0003276800000000001\n",
      "Epoch 7, Loss: 3.9695422649383545\n",
      "Current learning rate: 0.0003276800000000001\n",
      "Epoch 7, Loss: 3.1841750144958496\n",
      "Current learning rate: 0.0003276800000000001\n",
      "Epoch 7, Loss: 4.081778526306152\n",
      "Current learning rate: 0.0002621440000000001\n",
      "Epoch 7, Loss: 4.997181415557861\n",
      "Current learning rate: 0.0002621440000000001\n",
      "Epoch 7, Loss: 3.5232293605804443\n",
      "Current learning rate: 0.0002621440000000001\n",
      "Epoch 7, Loss: 2.92008113861084\n",
      "Current learning rate: 0.0002621440000000001\n",
      "Epoch 7, Loss: 2.9936511516571045\n",
      "Current learning rate: 0.0002621440000000001\n",
      "Epoch 7, Loss: 2.796102285385132\n",
      "Current learning rate: 0.0002621440000000001\n",
      "Epoch 7, Loss: 4.78561544418335\n",
      "Current learning rate: 0.0002621440000000001\n",
      "Epoch 7, Loss: 3.7142367362976074\n",
      "Current learning rate: 0.0002621440000000001\n",
      "Epoch 7, Loss: 3.6710286140441895\n",
      "Current learning rate: 0.0002621440000000001\n",
      "Epoch 7, Loss: 3.362684726715088\n",
      "Current learning rate: 0.0002621440000000001\n",
      "Epoch 7, Loss: 3.071082353591919\n",
      "Current learning rate: 0.0002621440000000001\n",
      "Epoch 7, Loss: 4.487100601196289\n",
      "Current learning rate: 0.00020971520000000012\n",
      "Epoch 8, Loss: 3.343024492263794\n",
      "Current learning rate: 0.00020971520000000012\n",
      "Epoch 8, Loss: 2.7751152515411377\n",
      "Current learning rate: 0.00020971520000000012\n",
      "Epoch 8, Loss: 3.5181849002838135\n",
      "Current learning rate: 0.00020971520000000012\n",
      "Epoch 8, Loss: 4.142058849334717\n",
      "Current learning rate: 0.00020971520000000012\n",
      "Epoch 8, Loss: 4.560735702514648\n",
      "Current learning rate: 0.00020971520000000012\n",
      "Epoch 8, Loss: 3.9120123386383057\n",
      "Current learning rate: 0.00020971520000000012\n",
      "Epoch 8, Loss: 2.8998117446899414\n",
      "Current learning rate: 0.00020971520000000012\n",
      "Epoch 8, Loss: 3.882297992706299\n",
      "Current learning rate: 0.00020971520000000012\n",
      "Epoch 8, Loss: 3.295182943344116\n",
      "Current learning rate: 0.00020971520000000012\n",
      "Epoch 8, Loss: 3.9617347717285156\n",
      "Current learning rate: 0.00020971520000000012\n",
      "Epoch 8, Loss: 4.330477714538574\n",
      "Current learning rate: 0.0001677721600000001\n",
      "Epoch 8, Loss: 4.465518951416016\n",
      "Current learning rate: 0.0001677721600000001\n",
      "Epoch 8, Loss: 4.986844539642334\n",
      "Current learning rate: 0.0001677721600000001\n",
      "Epoch 8, Loss: 2.6202046871185303\n",
      "Current learning rate: 0.0001677721600000001\n",
      "Epoch 8, Loss: 3.6662073135375977\n",
      "Current learning rate: 0.0001677721600000001\n",
      "Epoch 8, Loss: 2.820586681365967\n",
      "Current learning rate: 0.0001677721600000001\n",
      "Epoch 8, Loss: 3.707590341567993\n",
      "Current learning rate: 0.0001677721600000001\n",
      "Epoch 8, Loss: 4.764017105102539\n",
      "Current learning rate: 0.0001677721600000001\n",
      "Epoch 8, Loss: 3.2619476318359375\n",
      "Current learning rate: 0.0001677721600000001\n",
      "Epoch 8, Loss: 2.992457389831543\n",
      "Current learning rate: 0.0001677721600000001\n",
      "Epoch 8, Loss: 3.0743203163146973\n",
      "Current learning rate: 0.0001677721600000001\n",
      "Epoch 8, Loss: 4.080307960510254\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 8, Loss: 3.183228015899658\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 4.753203392028809\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 3.3578100204467773\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 3.1567881107330322\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 3.694145679473877\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 4.980129241943359\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 2.792358160018921\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 3.8897104263305664\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 3.494541645050049\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 2.5969789028167725\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 3.0531818866729736\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 2.969729423522949\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 4.069676399230957\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 4.139471054077148\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 4.550385475158691\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 4.328718185424805\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 4.4703898429870605\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 3.2464544773101807\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 3.6603002548217773\n",
      "Current learning rate: 0.00013421772800000008\n",
      "Epoch 9, Loss: 2.906585454940796\n",
      "Current learning rate: 0.00010737418240000007\n",
      "Epoch 9, Loss: 3.2920987606048584\n",
      "Current learning rate: 0.00010737418240000007\n",
      "Epoch 9, Loss: 2.7831716537475586\n",
      "Current learning rate: 0.00010737418240000007\n",
      "Epoch 9, Loss: 3.90147066116333\n",
      "Current learning rate: 0.00010737418240000007\n",
      "Epoch 9, Loss: 3.9546496868133545\n",
      "Current learning rate: 0.00010737418240000007\n",
      "Epoch 10, Loss: 4.76652717590332\n",
      "Current learning rate: 0.00010737418240000007\n",
      "Epoch 10, Loss: 4.529321670532227\n",
      "Current learning rate: 0.00010737418240000007\n",
      "Epoch 10, Loss: 3.642796516418457\n",
      "Current learning rate: 0.00010737418240000007\n",
      "Epoch 10, Loss: 2.6013216972351074\n",
      "Current learning rate: 0.00010737418240000007\n",
      "Epoch 10, Loss: 3.7026591300964355\n",
      "Current learning rate: 0.00010737418240000007\n",
      "Epoch 10, Loss: 4.448270797729492\n",
      "Current learning rate: 0.00010737418240000007\n",
      "Epoch 10, Loss: 4.311617374420166\n",
      "Current learning rate: 8.589934592000007e-05\n",
      "Epoch 10, Loss: 2.7800052165985107\n",
      "Current learning rate: 8.589934592000007e-05\n",
      "Epoch 10, Loss: 2.80094051361084\n",
      "Current learning rate: 8.589934592000007e-05\n",
      "Epoch 10, Loss: 3.5080010890960693\n",
      "Current learning rate: 8.589934592000007e-05\n",
      "Epoch 10, Loss: 4.9735283851623535\n",
      "Current learning rate: 8.589934592000007e-05\n",
      "Epoch 10, Loss: 2.9773809909820557\n",
      "Current learning rate: 8.589934592000007e-05\n",
      "Epoch 10, Loss: 4.124827861785889\n",
      "Current learning rate: 8.589934592000007e-05\n",
      "Epoch 10, Loss: 3.354607582092285\n",
      "Current learning rate: 8.589934592000007e-05\n",
      "Epoch 10, Loss: 3.857585906982422\n",
      "Current learning rate: 8.589934592000007e-05\n",
      "Epoch 10, Loss: 3.939387321472168\n",
      "Current learning rate: 8.589934592000007e-05\n",
      "Epoch 10, Loss: 4.067245960235596\n",
      "Current learning rate: 8.589934592000007e-05\n",
      "Epoch 10, Loss: 3.24819278717041\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 10, Loss: 3.0620739459991455\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 10, Loss: 2.9070310592651367\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 10, Loss: 3.8933541774749756\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 10, Loss: 3.2847440242767334\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 10, Loss: 3.1750495433807373\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 3.040292739868164\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 2.9610955715179443\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 2.7650277614593506\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 3.926255464553833\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 2.5871026515960693\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 2.7777488231658936\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 4.975776195526123\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 4.766129970550537\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 4.117587089538574\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 4.539848327636719\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 4.0608673095703125\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 3.6969027519226074\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 3.2704718112945557\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 4.4516730308532715\n",
      "Current learning rate: 6.871947673600006e-05\n",
      "Epoch 11, Loss: 3.344581365585327\n",
      "Current learning rate: 5.497558138880005e-05\n",
      "Epoch 11, Loss: 2.890467405319214\n",
      "Current learning rate: 5.497558138880005e-05\n",
      "Epoch 11, Loss: 3.4914052486419678\n",
      "Current learning rate: 5.497558138880005e-05\n",
      "Epoch 11, Loss: 4.316316604614258\n",
      "Current learning rate: 5.497558138880005e-05\n",
      "Epoch 11, Loss: 3.6499814987182617\n",
      "Current learning rate: 5.497558138880005e-05\n",
      "Epoch 11, Loss: 3.237576484680176\n",
      "Current learning rate: 5.497558138880005e-05\n",
      "Epoch 11, Loss: 3.1622819900512695\n",
      "Current learning rate: 5.497558138880005e-05\n",
      "Epoch 11, Loss: 3.8860397338867188\n",
      "Current learning rate: 5.497558138880005e-05\n",
      "Epoch 11, Loss: 3.8452677726745605\n",
      "Current learning rate: 5.497558138880005e-05\n",
      "Epoch 12, Loss: 4.510988235473633\n",
      "Current learning rate: 5.497558138880005e-05\n",
      "Epoch 12, Loss: 3.217282772064209\n",
      "Current learning rate: 5.497558138880005e-05\n",
      "Epoch 12, Loss: 3.6798760890960693\n",
      "Current learning rate: 4.3980465111040044e-05\n",
      "Epoch 12, Loss: 4.101672649383545\n",
      "Current learning rate: 4.3980465111040044e-05\n",
      "Epoch 12, Loss: 2.8835363388061523\n",
      "Current learning rate: 4.3980465111040044e-05\n",
      "Epoch 12, Loss: 4.9499030113220215\n",
      "Current learning rate: 4.3980465111040044e-05\n",
      "Epoch 12, Loss: 3.459707021713257\n",
      "Current learning rate: 4.3980465111040044e-05\n",
      "Epoch 12, Loss: 2.7893552780151367\n",
      "Current learning rate: 4.3980465111040044e-05\n",
      "Epoch 12, Loss: 3.2665512561798096\n",
      "Current learning rate: 4.3980465111040044e-05\n",
      "Epoch 12, Loss: 3.150306463241577\n",
      "Current learning rate: 4.3980465111040044e-05\n",
      "Epoch 12, Loss: 2.9681482315063477\n",
      "Current learning rate: 4.3980465111040044e-05\n",
      "Epoch 12, Loss: 3.042855978012085\n",
      "Current learning rate: 4.3980465111040044e-05\n",
      "Epoch 12, Loss: 3.809626340866089\n",
      "Current learning rate: 4.3980465111040044e-05\n",
      "Epoch 12, Loss: 4.054265022277832\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 12, Loss: 3.3369343280792236\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 12, Loss: 3.635211944580078\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 12, Loss: 3.871370553970337\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 12, Loss: 4.297077655792236\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 12, Loss: 2.767899990081787\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 12, Loss: 2.590649127960205\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 12, Loss: 4.745327472686768\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 12, Loss: 4.435294151306152\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 12, Loss: 3.9267327785491943\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 13, Loss: 3.218122959136963\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 13, Loss: 2.574584722518921\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 13, Loss: 3.025869846343994\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 13, Loss: 4.034159183502197\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 13, Loss: 4.094313144683838\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 13, Loss: 2.774001121520996\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 13, Loss: 4.71562385559082\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 13, Loss: 2.9477720260620117\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 13, Loss: 3.4479033946990967\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 13, Loss: 3.619704246520996\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 13, Loss: 3.903575897216797\n",
      "Current learning rate: 3.5184372088832036e-05\n",
      "Epoch 13, Loss: 3.1414637565612793\n",
      "Current learning rate: 2.814749767106563e-05\n",
      "Epoch 13, Loss: 3.3241560459136963\n",
      "Current learning rate: 2.814749767106563e-05\n",
      "Epoch 13, Loss: 4.414700508117676\n",
      "Current learning rate: 2.814749767106563e-05\n",
      "Epoch 13, Loss: 2.884061098098755\n",
      "Current learning rate: 2.814749767106563e-05\n",
      "Epoch 13, Loss: 3.6784274578094482\n",
      "Current learning rate: 2.814749767106563e-05\n",
      "Epoch 13, Loss: 3.801476001739502\n",
      "Current learning rate: 2.814749767106563e-05\n",
      "Epoch 13, Loss: 4.507348537445068\n",
      "Current learning rate: 2.814749767106563e-05\n",
      "Epoch 13, Loss: 4.946147918701172\n",
      "Current learning rate: 2.814749767106563e-05\n",
      "Epoch 13, Loss: 4.285816669464111\n",
      "Current learning rate: 2.814749767106563e-05\n",
      "Epoch 13, Loss: 3.263263463973999\n",
      "Current learning rate: 2.814749767106563e-05\n",
      "Epoch 13, Loss: 2.7595951557159424\n",
      "Current learning rate: 2.814749767106563e-05\n",
      "Epoch 13, Loss: 3.8627302646636963\n",
      "Current learning rate: 2.2517998136852506e-05\n",
      "Epoch 14, Loss: 3.6093220710754395\n",
      "Current learning rate: 2.2517998136852506e-05\n",
      "Epoch 14, Loss: 3.6580402851104736\n",
      "Current learning rate: 2.2517998136852506e-05\n",
      "Epoch 14, Loss: 2.574756383895874\n",
      "Current learning rate: 2.2517998136852506e-05\n",
      "Epoch 14, Loss: 4.080057144165039\n",
      "Current learning rate: 2.2517998136852506e-05\n",
      "Epoch 14, Loss: 3.8913562297821045\n",
      "Current learning rate: 2.2517998136852506e-05\n",
      "Epoch 14, Loss: 2.8754167556762695\n",
      "Current learning rate: 2.2517998136852506e-05\n",
      "Epoch 14, Loss: 4.7037787437438965\n",
      "Current learning rate: 2.2517998136852506e-05\n",
      "Epoch 14, Loss: 4.2639265060424805\n",
      "Current learning rate: 2.2517998136852506e-05\n",
      "Epoch 14, Loss: 3.31520938873291\n",
      "Current learning rate: 2.2517998136852506e-05\n",
      "Epoch 14, Loss: 4.033069133758545\n",
      "Current learning rate: 2.2517998136852506e-05\n",
      "Epoch 14, Loss: 3.021155834197998\n",
      "Current learning rate: 1.8014398509482006e-05\n",
      "Epoch 14, Loss: 4.480143070220947\n",
      "Current learning rate: 1.8014398509482006e-05\n",
      "Epoch 14, Loss: 4.39619779586792\n",
      "Current learning rate: 1.8014398509482006e-05\n",
      "Epoch 14, Loss: 3.2171757221221924\n",
      "Current learning rate: 1.8014398509482006e-05\n",
      "Epoch 14, Loss: 3.248461961746216\n",
      "Current learning rate: 1.8014398509482006e-05\n",
      "Epoch 14, Loss: 2.7765612602233887\n",
      "Current learning rate: 1.8014398509482006e-05\n",
      "Epoch 14, Loss: 4.9188337326049805\n",
      "Current learning rate: 1.8014398509482006e-05\n",
      "Epoch 14, Loss: 3.1374220848083496\n",
      "Current learning rate: 1.8014398509482006e-05\n",
      "Epoch 14, Loss: 3.8454623222351074\n",
      "Current learning rate: 1.8014398509482006e-05\n",
      "Epoch 14, Loss: 3.8000407218933105\n",
      "Current learning rate: 1.8014398509482006e-05\n",
      "Epoch 14, Loss: 3.4326751232147217\n",
      "Current learning rate: 1.8014398509482006e-05\n",
      "Epoch 14, Loss: 2.948185682296753\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 14, Loss: 2.7545745372772217\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 2.7504665851593018\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 3.123441696166992\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 2.870084762573242\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 4.0180439949035645\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 2.7614290714263916\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 3.8319735527038574\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 3.604609489440918\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 3.410747766494751\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 2.5654118061065674\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 3.654296875\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 4.692079544067383\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 4.908888816833496\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 3.005990982055664\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 4.26382303237915\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 3.8857531547546387\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 4.474028587341309\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 4.38801383972168\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 3.206866979598999\n",
      "Current learning rate: 1.4411518807585605e-05\n",
      "Epoch 15, Loss: 3.242635726928711\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 15, Loss: 4.0728583335876465\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 15, Loss: 3.768441915512085\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 15, Loss: 2.9351024627685547\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 15, Loss: 3.311291217803955\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 16, Loss: 2.7351677417755127\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 16, Loss: 2.5582542419433594\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 16, Loss: 3.6433589458465576\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 16, Loss: 4.057154178619385\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 16, Loss: 4.012475490570068\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 16, Loss: 3.1183056831359863\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 16, Loss: 2.9234373569488525\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 16, Loss: 3.297941207885742\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 16, Loss: 4.372636318206787\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 16, Loss: 3.7586796283721924\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 16, Loss: 2.9979333877563477\n",
      "Current learning rate: 1.1529215046068485e-05\n",
      "Epoch 16, Loss: 3.829357147216797\n",
      "Current learning rate: 9.223372036854789e-06\n",
      "Epoch 16, Loss: 4.8998122215271\n",
      "Current learning rate: 9.223372036854789e-06\n",
      "Epoch 16, Loss: 3.8754053115844727\n",
      "Current learning rate: 9.223372036854789e-06\n",
      "Epoch 16, Loss: 3.5965793132781982\n",
      "Current learning rate: 9.223372036854789e-06\n",
      "Epoch 16, Loss: 3.387960195541382\n",
      "Current learning rate: 9.223372036854789e-06\n",
      "Epoch 16, Loss: 4.680539131164551\n",
      "Current learning rate: 9.223372036854789e-06\n",
      "Epoch 16, Loss: 3.1991870403289795\n",
      "Current learning rate: 9.223372036854789e-06\n",
      "Epoch 16, Loss: 2.759077310562134\n",
      "Current learning rate: 9.223372036854789e-06\n",
      "Epoch 16, Loss: 2.8689651489257812\n",
      "Current learning rate: 9.223372036854789e-06\n",
      "Epoch 16, Loss: 4.255478858947754\n",
      "Current learning rate: 9.223372036854789e-06\n",
      "Epoch 16, Loss: 4.463464736938477\n",
      "Current learning rate: 9.223372036854789e-06\n",
      "Epoch 16, Loss: 3.237027168273926\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 4.666836261749268\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 3.6374502182006836\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 4.362318992614746\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 3.585118532180786\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 3.7440876960754395\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 3.2923316955566406\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 2.555799722671509\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 4.050021171569824\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 2.919870138168335\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 4.242857933044434\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 2.861721992492676\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 2.753413677215576\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 4.007845401763916\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 3.228538990020752\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 3.3790283203125\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 2.9922916889190674\n",
      "Current learning rate: 7.378697629483831e-06\n",
      "Epoch 17, Loss: 3.1149673461914062\n",
      "Current learning rate: 5.902958103587065e-06\n",
      "Epoch 17, Loss: 4.452626705169678\n",
      "Current learning rate: 5.902958103587065e-06\n",
      "Epoch 17, Loss: 4.890641212463379\n",
      "Current learning rate: 5.902958103587065e-06\n",
      "Epoch 17, Loss: 3.1919124126434326\n",
      "Current learning rate: 5.902958103587065e-06\n",
      "Epoch 17, Loss: 3.869645118713379\n",
      "Current learning rate: 5.902958103587065e-06\n",
      "Epoch 17, Loss: 2.7319893836975098\n",
      "Current learning rate: 5.902958103587065e-06\n",
      "Epoch 17, Loss: 3.823885202407837\n",
      "Current learning rate: 5.902958103587065e-06\n",
      "Epoch 18, Loss: 3.2863245010375977\n",
      "Current learning rate: 5.902958103587065e-06\n",
      "Epoch 18, Loss: 3.221170663833618\n",
      "Current learning rate: 5.902958103587065e-06\n",
      "Epoch 18, Loss: 3.999608278274536\n",
      "Current learning rate: 5.902958103587065e-06\n",
      "Epoch 18, Loss: 3.581322193145752\n",
      "Current learning rate: 5.902958103587065e-06\n",
      "Epoch 18, Loss: 3.736292600631714\n",
      "Current learning rate: 4.722366482869652e-06\n",
      "Epoch 18, Loss: 4.2360405921936035\n",
      "Current learning rate: 4.722366482869652e-06\n",
      "Epoch 18, Loss: 3.8173952102661133\n",
      "Current learning rate: 4.722366482869652e-06\n",
      "Epoch 18, Loss: 2.9148435592651367\n",
      "Current learning rate: 4.722366482869652e-06\n",
      "Epoch 18, Loss: 4.044775009155273\n",
      "Current learning rate: 4.722366482869652e-06\n",
      "Epoch 18, Loss: 3.3721375465393066\n",
      "Current learning rate: 4.722366482869652e-06\n",
      "Epoch 18, Loss: 3.1863291263580322\n",
      "Current learning rate: 4.722366482869652e-06\n",
      "Epoch 18, Loss: 4.881987571716309\n",
      "Current learning rate: 4.722366482869652e-06\n",
      "Epoch 18, Loss: 4.663486480712891\n",
      "Current learning rate: 4.722366482869652e-06\n",
      "Epoch 18, Loss: 3.1095476150512695\n",
      "Current learning rate: 4.722366482869652e-06\n",
      "Epoch 18, Loss: 3.8623080253601074\n",
      "Current learning rate: 4.722366482869652e-06\n",
      "Epoch 18, Loss: 3.6357998847961426\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 18, Loss: 2.751270294189453\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 18, Loss: 4.359045028686523\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 18, Loss: 2.7290916442871094\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 18, Loss: 2.8597452640533447\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 18, Loss: 2.5522918701171875\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 18, Loss: 2.987726926803589\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 18, Loss: 4.446855068206787\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 3.57680344581604\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 3.995941162109375\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 4.875299453735352\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 2.8559961318969727\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 3.811131477355957\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 3.285520076751709\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 2.548377513885498\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 3.1834328174591064\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 3.371011734008789\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 3.630615711212158\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 2.7471485137939453\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 3.85872483253479\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 4.657832145690918\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 3.7306294441223145\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 3.106088399887085\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 2.7253541946411133\n",
      "Current learning rate: 3.777893186295722e-06\n",
      "Epoch 19, Loss: 4.043638229370117\n",
      "Current learning rate: 3.022314549036578e-06\n",
      "Epoch 19, Loss: 4.443575859069824\n",
      "Current learning rate: 3.022314549036578e-06\n",
      "Epoch 19, Loss: 2.9835739135742188\n",
      "Current learning rate: 3.022314549036578e-06\n",
      "Epoch 19, Loss: 2.913985013961792\n",
      "Current learning rate: 3.022314549036578e-06\n",
      "Epoch 19, Loss: 3.2231736183166504\n",
      "Current learning rate: 3.022314549036578e-06\n",
      "Epoch 19, Loss: 4.238862991333008\n",
      "Current learning rate: 3.022314549036578e-06\n",
      "Epoch 19, Loss: 4.35695743560791\n",
      "Current learning rate: 3.022314549036578e-06\n",
      "Epoch 20, Loss: 2.7223095893859863\n",
      "Current learning rate: 3.022314549036578e-06\n",
      "Epoch 20, Loss: 3.9947657585144043\n",
      "Current learning rate: 3.022314549036578e-06\n",
      "Epoch 20, Loss: 3.8548758029937744\n",
      "Current learning rate: 3.022314549036578e-06\n",
      "Epoch 20, Loss: 3.18094539642334\n",
      "Current learning rate: 3.022314549036578e-06\n",
      "Epoch 20, Loss: 3.726534605026245\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 4.235086441040039\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 2.7446060180664062\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 2.85315203666687\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 3.3650388717651367\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 4.875063896179199\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 3.576570749282837\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 3.8117456436157227\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 3.103863000869751\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 3.219879627227783\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 2.981029987335205\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 3.6294021606445312\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 2.5460259914398193\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 4.656125068664551\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 2.911505699157715\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 4.353452682495117\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 3.2850406169891357\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 4.441438674926758\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 20, Loss: 4.041369438171387\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 21, Loss: 3.101323366165161\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 21, Loss: 3.2171859741210938\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 21, Loss: 2.851428270339966\n",
      "Current learning rate: 2.4178516392292624e-06\n",
      "Epoch 21, Loss: 4.039224147796631\n",
      "Current learning rate: 1.93428131138341e-06\n",
      "Epoch 21, Loss: 3.6267712116241455\n",
      "Current learning rate: 1.93428131138341e-06\n",
      "Epoch 21, Loss: 3.180189371109009\n",
      "Current learning rate: 1.93428131138341e-06\n",
      "Epoch 21, Loss: 2.543879985809326\n",
      "Current learning rate: 1.93428131138341e-06\n",
      "Epoch 21, Loss: 3.725165367126465\n",
      "Current learning rate: 1.93428131138341e-06\n",
      "Epoch 21, Loss: 2.909489154815674\n",
      "Current learning rate: 1.93428131138341e-06\n",
      "Epoch 21, Loss: 3.57464337348938\n",
      "Current learning rate: 1.93428131138341e-06\n",
      "Epoch 21, Loss: 4.43880558013916\n",
      "Current learning rate: 1.93428131138341e-06\n",
      "Epoch 21, Loss: 2.721428155899048\n",
      "Current learning rate: 1.93428131138341e-06\n",
      "Epoch 21, Loss: 3.3639955520629883\n",
      "Current learning rate: 1.93428131138341e-06\n",
      "Epoch 21, Loss: 2.7442426681518555\n",
      "Current learning rate: 1.93428131138341e-06\n",
      "Epoch 21, Loss: 4.874307632446289\n",
      "Current learning rate: 1.93428131138341e-06\n",
      "Epoch 21, Loss: 3.855987071990967\n",
      "Current learning rate: 1.93428131138341e-06\n",
      "Epoch 21, Loss: 3.996028423309326\n",
      "Current learning rate: 1.547425049106728e-06\n",
      "Epoch 21, Loss: 4.234102249145508\n",
      "Current learning rate: 1.547425049106728e-06\n",
      "Epoch 21, Loss: 4.65399694442749\n",
      "Current learning rate: 1.547425049106728e-06\n",
      "Epoch 21, Loss: 4.351589202880859\n",
      "Current learning rate: 1.547425049106728e-06\n",
      "Epoch 21, Loss: 2.979814052581787\n",
      "Current learning rate: 1.547425049106728e-06\n",
      "Epoch 21, Loss: 3.811173439025879\n",
      "Current learning rate: 1.547425049106728e-06\n",
      "Epoch 21, Loss: 3.283205032348633\n",
      "Current learning rate: 1.547425049106728e-06\n",
      "Epoch 22, Loss: 4.232358932495117\n",
      "Current learning rate: 1.547425049106728e-06\n",
      "Epoch 22, Loss: 3.8537685871124268\n",
      "Current learning rate: 1.547425049106728e-06\n",
      "Epoch 22, Loss: 3.36210036277771\n",
      "Current learning rate: 1.547425049106728e-06\n",
      "Epoch 22, Loss: 2.8508353233337402\n",
      "Current learning rate: 1.547425049106728e-06\n",
      "Epoch 22, Loss: 4.871282577514648\n",
      "Current learning rate: 1.2379400392853825e-06\n",
      "Epoch 22, Loss: 4.3493571281433105\n",
      "Current learning rate: 1.2379400392853825e-06\n",
      "Epoch 22, Loss: 3.2163970470428467\n",
      "Current learning rate: 1.2379400392853825e-06\n",
      "Epoch 22, Loss: 3.7219386100769043\n",
      "Current learning rate: 1.2379400392853825e-06\n",
      "Epoch 22, Loss: 2.7436716556549072\n",
      "Current learning rate: 1.2379400392853825e-06\n",
      "Epoch 22, Loss: 4.03555965423584\n",
      "Current learning rate: 1.2379400392853825e-06\n",
      "Epoch 22, Loss: 4.651489734649658\n",
      "Current learning rate: 1.2379400392853825e-06\n",
      "Epoch 22, Loss: 2.9088401794433594\n",
      "Current learning rate: 1.2379400392853825e-06\n",
      "Epoch 22, Loss: 3.9940311908721924\n",
      "Current learning rate: 1.2379400392853825e-06\n",
      "Epoch 22, Loss: 3.2823193073272705\n",
      "Current learning rate: 1.2379400392853825e-06\n",
      "Epoch 22, Loss: 3.8093364238739014\n",
      "Current learning rate: 1.2379400392853825e-06\n",
      "Epoch 22, Loss: 4.43671989440918\n",
      "Current learning rate: 9.90352031428306e-07\n",
      "Epoch 22, Loss: 3.6264774799346924\n",
      "Current learning rate: 9.90352031428306e-07\n",
      "Epoch 22, Loss: 2.979461669921875\n",
      "Current learning rate: 9.90352031428306e-07\n",
      "Epoch 22, Loss: 2.544163703918457\n",
      "Current learning rate: 9.90352031428306e-07\n",
      "Epoch 22, Loss: 3.5743725299835205\n",
      "Current learning rate: 9.90352031428306e-07\n",
      "Epoch 22, Loss: 3.1809873580932617\n",
      "Current learning rate: 9.90352031428306e-07\n",
      "Epoch 22, Loss: 2.721811532974243\n",
      "Current learning rate: 9.90352031428306e-07\n",
      "Epoch 22, Loss: 3.1024351119995117\n",
      "Current learning rate: 9.90352031428306e-07\n",
      "Epoch 23, Loss: 2.9079456329345703\n",
      "Current learning rate: 9.90352031428306e-07\n",
      "Epoch 23, Loss: 3.180305004119873\n",
      "Current learning rate: 9.90352031428306e-07\n",
      "Epoch 23, Loss: 4.229854106903076\n",
      "Current learning rate: 9.90352031428306e-07\n",
      "Epoch 23, Loss: 3.5733463764190674\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 4.649507522583008\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 3.8521978855133057\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 3.101422071456909\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 4.3475542068481445\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 3.807844400405884\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 2.8509926795959473\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 2.543246269226074\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 2.9782519340515137\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 3.281157970428467\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 4.870357513427734\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 4.436007976531982\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 2.7433042526245117\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 4.035552978515625\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 3.722705364227295\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 3.362251043319702\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 2.7209312915802\n",
      "Current learning rate: 7.922816251426449e-07\n",
      "Epoch 23, Loss: 3.993687868118286\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 23, Loss: 3.6260385513305664\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 23, Loss: 3.2170655727386475\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 3.5717005729675293\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 3.85164475440979\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 3.72152042388916\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 2.7203242778778076\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 3.1000733375549316\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 3.992915630340576\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 3.2165493965148926\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 3.360581398010254\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 4.034785270690918\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 2.5424859523773193\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 2.850698947906494\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 3.2807207107543945\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 2.9076342582702637\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 3.8077855110168457\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 3.1791231632232666\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 2.9776344299316406\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 3.625516176223755\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 2.742898464202881\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 4.348203182220459\n",
      "Current learning rate: 6.338253001141159e-07\n",
      "Epoch 24, Loss: 4.650356292724609\n",
      "Current learning rate: 5.070602400912927e-07\n",
      "Epoch 24, Loss: 4.436223983764648\n",
      "Current learning rate: 5.070602400912927e-07\n",
      "Epoch 24, Loss: 4.870734691619873\n",
      "Current learning rate: 5.070602400912927e-07\n",
      "Epoch 24, Loss: 4.231264591217041\n",
      "Current learning rate: 5.070602400912927e-07\n",
      "Epoch 25, Loss: 3.099522352218628\n",
      "Current learning rate: 5.070602400912927e-07\n",
      "Epoch 25, Loss: 3.6247596740722656\n",
      "Current learning rate: 5.070602400912927e-07\n",
      "Epoch 25, Loss: 3.178522825241089\n",
      "Current learning rate: 5.070602400912927e-07\n",
      "Epoch 25, Loss: 3.2800955772399902\n",
      "Current learning rate: 5.070602400912927e-07\n",
      "Epoch 25, Loss: 3.991884469985962\n",
      "Current learning rate: 5.070602400912927e-07\n",
      "Epoch 25, Loss: 4.034111022949219\n",
      "Current learning rate: 5.070602400912927e-07\n",
      "Epoch 25, Loss: 2.7424283027648926\n",
      "Current learning rate: 5.070602400912927e-07\n",
      "Epoch 25, Loss: 2.9771244525909424\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 2.7194900512695312\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 3.215590238571167\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 2.907270908355713\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 4.649652481079102\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 4.230618000030518\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 2.8502748012542725\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 3.8074450492858887\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 4.3477349281311035\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 3.5721538066864014\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 3.720999002456665\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 2.54221248626709\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 3.8523550033569336\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 4.8702263832092285\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 3.36055850982666\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 25, Loss: 4.435853958129883\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 26, Loss: 3.623906135559082\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 26, Loss: 4.869754314422607\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 26, Loss: 3.7205679416656494\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 26, Loss: 3.5717477798461914\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 26, Loss: 2.850052833557129\n",
      "Current learning rate: 4.0564819207303424e-07\n",
      "Epoch 26, Loss: 3.991664409637451\n",
      "Current learning rate: 3.245185536584274e-07\n",
      "Epoch 26, Loss: 4.229963302612305\n",
      "Current learning rate: 3.245185536584274e-07\n",
      "Epoch 26, Loss: 2.9768834114074707\n",
      "Current learning rate: 3.245185536584274e-07\n",
      "Epoch 26, Loss: 2.9071731567382812\n",
      "Current learning rate: 3.245185536584274e-07\n",
      "Epoch 26, Loss: 3.807067394256592\n",
      "Current learning rate: 3.245185536584274e-07\n",
      "Epoch 26, Loss: 4.649023532867432\n",
      "Current learning rate: 3.245185536584274e-07\n",
      "Epoch 26, Loss: 3.1783668994903564\n",
      "Current learning rate: 3.245185536584274e-07\n",
      "Epoch 26, Loss: 2.7196061611175537\n",
      "Current learning rate: 3.245185536584274e-07\n",
      "Epoch 26, Loss: 3.215625524520874\n",
      "Current learning rate: 3.245185536584274e-07\n",
      "Epoch 26, Loss: 3.280209541320801\n",
      "Current learning rate: 3.245185536584274e-07\n",
      "Epoch 26, Loss: 3.359769344329834\n",
      "Current learning rate: 3.245185536584274e-07\n",
      "Epoch 26, Loss: 4.347379684448242\n",
      "Current learning rate: 2.5961484292674195e-07\n",
      "Epoch 26, Loss: 4.435309410095215\n",
      "Current learning rate: 2.5961484292674195e-07\n",
      "Epoch 26, Loss: 3.0999908447265625\n",
      "Current learning rate: 2.5961484292674195e-07\n",
      "Epoch 26, Loss: 2.5422604084014893\n",
      "Current learning rate: 2.5961484292674195e-07\n",
      "Epoch 26, Loss: 3.8521008491516113\n",
      "Current learning rate: 2.5961484292674195e-07\n",
      "Epoch 26, Loss: 2.7425451278686523\n",
      "Current learning rate: 2.5961484292674195e-07\n",
      "Epoch 26, Loss: 4.034448623657227\n",
      "Current learning rate: 2.5961484292674195e-07\n",
      "Epoch 27, Loss: 3.359513282775879\n",
      "Current learning rate: 2.5961484292674195e-07\n",
      "Epoch 27, Loss: 3.851881265640259\n",
      "Current learning rate: 2.5961484292674195e-07\n",
      "Epoch 27, Loss: 4.86843729019165\n",
      "Current learning rate: 2.5961484292674195e-07\n",
      "Epoch 27, Loss: 4.229537487030029\n",
      "Current learning rate: 2.5961484292674195e-07\n",
      "Epoch 27, Loss: 3.571279764175415\n",
      "Current learning rate: 2.0769187434139356e-07\n",
      "Epoch 27, Loss: 4.346994400024414\n",
      "Current learning rate: 2.0769187434139356e-07\n",
      "Epoch 27, Loss: 4.648641586303711\n",
      "Current learning rate: 2.0769187434139356e-07\n",
      "Epoch 27, Loss: 3.806771993637085\n",
      "Current learning rate: 2.0769187434139356e-07\n",
      "Epoch 27, Loss: 3.0998079776763916\n",
      "Current learning rate: 2.0769187434139356e-07\n",
      "Epoch 27, Loss: 2.9768099784851074\n",
      "Current learning rate: 2.0769187434139356e-07\n",
      "Epoch 27, Loss: 3.9916088581085205\n",
      "Current learning rate: 2.0769187434139356e-07\n",
      "Epoch 27, Loss: 2.542114019393921\n",
      "Current learning rate: 2.0769187434139356e-07\n",
      "Epoch 27, Loss: 2.907121181488037\n",
      "Current learning rate: 2.0769187434139356e-07\n",
      "Epoch 27, Loss: 3.215536594390869\n",
      "Current learning rate: 2.0769187434139356e-07\n",
      "Epoch 27, Loss: 2.8501381874084473\n",
      "Current learning rate: 2.0769187434139356e-07\n",
      "Epoch 27, Loss: 3.7196261882781982\n",
      "Current learning rate: 1.6615349947311486e-07\n",
      "Epoch 27, Loss: 2.742476463317871\n",
      "Current learning rate: 1.6615349947311486e-07\n",
      "Epoch 27, Loss: 3.624180793762207\n",
      "Current learning rate: 1.6615349947311486e-07\n",
      "Epoch 27, Loss: 3.280200242996216\n",
      "Current learning rate: 1.6615349947311486e-07\n",
      "Epoch 27, Loss: 4.034269332885742\n",
      "Current learning rate: 1.6615349947311486e-07\n",
      "Epoch 27, Loss: 4.435075759887695\n",
      "Current learning rate: 1.6615349947311486e-07\n",
      "Epoch 27, Loss: 3.1784417629241943\n",
      "Current learning rate: 1.6615349947311486e-07\n",
      "Epoch 27, Loss: 2.7196249961853027\n",
      "Current learning rate: 1.6615349947311486e-07\n",
      "Epoch 28, Loss: 3.8065261840820312\n",
      "Current learning rate: 1.6615349947311486e-07\n",
      "Epoch 28, Loss: 2.7422914505004883\n",
      "Current learning rate: 1.6615349947311486e-07\n",
      "Epoch 28, Loss: 3.099534273147583\n",
      "Current learning rate: 1.6615349947311486e-07\n",
      "Epoch 28, Loss: 2.9069221019744873\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 4.346660137176514\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 4.6484270095825195\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 3.1782736778259277\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 2.5418660640716553\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 3.991486072540283\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 4.229490280151367\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 4.034051895141602\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 4.434890270233154\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 3.7196943759918213\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 4.868463516235352\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 3.851414203643799\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 2.8500144481658936\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 3.280073404312134\n",
      "Current learning rate: 1.329227995784919e-07\n",
      "Epoch 28, Loss: 3.6241095066070557\n",
      "Current learning rate: 1.0633823966279352e-07\n",
      "Epoch 28, Loss: 3.359405517578125\n",
      "Current learning rate: 1.0633823966279352e-07\n",
      "Epoch 28, Loss: 2.9767816066741943\n",
      "Current learning rate: 1.0633823966279352e-07\n",
      "Epoch 28, Loss: 3.5713610649108887\n",
      "Current learning rate: 1.0633823966279352e-07\n",
      "Epoch 28, Loss: 2.719533681869507\n",
      "Current learning rate: 1.0633823966279352e-07\n",
      "Epoch 28, Loss: 3.215494155883789\n",
      "Current learning rate: 1.0633823966279352e-07\n",
      "Epoch 29, Loss: 3.9913265705108643\n",
      "Current learning rate: 1.0633823966279352e-07\n",
      "Epoch 29, Loss: 4.033824920654297\n",
      "Current learning rate: 1.0633823966279352e-07\n",
      "Epoch 29, Loss: 3.62398362159729\n",
      "Current learning rate: 1.0633823966279352e-07\n",
      "Epoch 29, Loss: 4.868288040161133\n",
      "Current learning rate: 1.0633823966279352e-07\n",
      "Epoch 29, Loss: 3.8064165115356445\n",
      "Current learning rate: 1.0633823966279352e-07\n",
      "Epoch 29, Loss: 2.719451904296875\n",
      "Current learning rate: 8.507059173023481e-08\n",
      "Epoch 29, Loss: 2.7421183586120605\n",
      "Current learning rate: 8.507059173023481e-08\n",
      "Epoch 29, Loss: 3.178093433380127\n",
      "Current learning rate: 8.507059173023481e-08\n",
      "Epoch 29, Loss: 3.8513128757476807\n",
      "Current learning rate: 8.507059173023481e-08\n",
      "Epoch 29, Loss: 4.2294158935546875\n",
      "Current learning rate: 8.507059173023481e-08\n",
      "Epoch 29, Loss: 2.906869411468506\n",
      "Current learning rate: 8.507059173023481e-08\n",
      "Epoch 29, Loss: 4.346611976623535\n",
      "Current learning rate: 8.507059173023481e-08\n",
      "Epoch 29, Loss: 3.099505662918091\n",
      "Current learning rate: 8.507059173023481e-08\n",
      "Epoch 29, Loss: 4.648427486419678\n",
      "Current learning rate: 8.507059173023481e-08\n",
      "Epoch 29, Loss: 2.8499581813812256\n",
      "Current learning rate: 8.507059173023481e-08\n",
      "Epoch 29, Loss: 4.434725284576416\n",
      "Current learning rate: 8.507059173023481e-08\n",
      "Epoch 29, Loss: 3.2154102325439453\n",
      "Current learning rate: 6.805647338418785e-08\n",
      "Epoch 29, Loss: 2.541865825653076\n",
      "Current learning rate: 6.805647338418785e-08\n",
      "Epoch 29, Loss: 2.976726770401001\n",
      "Current learning rate: 6.805647338418785e-08\n",
      "Epoch 29, Loss: 3.359433650970459\n",
      "Current learning rate: 6.805647338418785e-08\n",
      "Epoch 29, Loss: 3.5713069438934326\n",
      "Current learning rate: 6.805647338418785e-08\n",
      "Epoch 29, Loss: 3.280029535293579\n",
      "Current learning rate: 6.805647338418785e-08\n",
      "Epoch 29, Loss: 3.7196743488311768\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class RetinaDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name)\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        # Genera le coordinate normalizzate per ogni pixel\n",
    "        coords = np.meshgrid(np.linspace(-1, 1, image.shape[2]), np.linspace(-1, 1, image.shape[1]))\n",
    "        coords = np.stack(coords, axis=-1)\n",
    "        coords = torch.FloatTensor(coords)  # Converte in tensori di torch\n",
    "        coords = coords.view(-1, 2)  # Flatten delle coordinate in [N, 2] dove N  il numero di pixel\n",
    "\n",
    "        return coords, image.view(-1, 1), mask.view(-1, 1)\n",
    "\n",
    "class SineLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features \n",
    "        self.omega_0 = omega_0\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        self.init_weights(is_first)\n",
    "\n",
    "    def init_weights(self, is_first):\n",
    "        with torch.no_grad():\n",
    "            if is_first:\n",
    "                self.linear.weight.uniform_(-1 / self.in_features, 1 / self.in_features)\n",
    "            else:\n",
    "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n",
    "                                            np.sqrt(6 / self.in_features) / self.omega_0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.omega_0 * self.linear(x))\n",
    "\n",
    "class Siren(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, hidden_layers, out_features, first_omega_0=30, hidden_omega_0=30):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([SineLayer(in_features, hidden_features, is_first=True, omega_0=first_omega_0)])\n",
    "        for _ in range(hidden_layers):\n",
    "            self.layers.append(SineLayer(hidden_features, hidden_features, omega_0=hidden_omega_0))\n",
    "        self.final_layer = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.final_layer(x)\n",
    "\n",
    "def train(model, dataloader, optimizer, scheduler, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for coords, image, mask in dataloader:\n",
    "            coords, image, mask = coords.to(device), image.to(device), mask.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(coords)\n",
    "            loss = nn.MSELoss()(outputs, mask)  # Calcola la perdita usando la maschera come target\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)  # Passa la perdita al scheduler\n",
    "            print(\"Current learning rate:\", scheduler.optimizer.param_groups[0]['lr'])\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_input = 'C:/Users/Q540900/Desktop/A.I. Master/Werkstudent/RAVIR Dataset/train/training_images'\n",
    "train_output = 'C:/Users/Q540900/Desktop/A.I. Master/Werkstudent/RAVIR Dataset/train/training_masks'\n",
    "\n",
    "transform = Compose([\n",
    "    # Resize((768, 768)), #256\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4227], std=[0.1457])\n",
    "])\n",
    "\n",
    "train_dataset = RetinaDataset(train_input, train_output, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "2843\n",
    "# Model\n",
    "model = Siren(in_features=2, hidden_features=768, hidden_layers=3, out_features=1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.8)\n",
    "\n",
    "# Training\n",
    "train(model, train_loader, optimizer, scheduler, epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'INR_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlnnl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
